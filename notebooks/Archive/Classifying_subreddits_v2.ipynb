{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Subreddit Posts <br>\n",
    "\n",
    "_**Author:** Bala Krishnamoorthy_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview <br>\n",
    "\n",
    "**Goal:** Build a model that can classify reddit posts into the subreddit they belong to. <br>\n",
    "\n",
    "**Note:** Throughout this workbook, you'll find my comments/insights on the workflow in _italics_. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology <br>\n",
    "\n",
    "To do this, I will build and compare two classifiers: **Logistic Regression** and **RandomForest**. Each classifier will rely on natural language processing (NLP) on the text within each post to better understand the characteristics (and ideally context) of the post. By doing so, the classifier will learn which post belongs in which subreddit. \n",
    "\n",
    "Here is an overview of the steps taken to build each classifier: <br>\n",
    "\n",
    "Steps common to both classifiers: \n",
    "- Pull posts from subreddits being examined using Reddit's API\n",
    "- Clean gathered data to extract post content (text), and any other potential identifying characteristics within each post. <br>\n",
    "- NLP: <br>\n",
    "    - Tokenize & lemmatize/stem data\n",
    "    - Vectorize data (CountVectorizer, HasingVectorizer, TF-IDF)\n",
    "- Modelling (Logisitic Regression, Random Forest)\n",
    "- Evaluate Model (initial)\n",
    "- Changes + Hyperparameter tuning: GridSearch, others? <br>\n",
    "- Evaluate Model (final)\n",
    "\n",
    "\n",
    "Logistic Regression: <br>\n",
    "\n",
    "Random Forest: <br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "## Machine Learning: sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier,\n",
    "VotingClassifier\n",
    "\n",
    "# Machine Learning: nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "## JSON Manipultion / API Access\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## Time \n",
    "import time\n",
    "\n",
    "## Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs={'facecolor' : \"w\"} # For dark themed j-notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pulling posts from subreddits\n",
    "\n",
    "# ## Set timer\n",
    "# t0 = time.time()\n",
    "\n",
    "# # Define parameters used in for-loop below\n",
    "# # Subreddits: 'technology/', 'fitness/', 'sports/', 'showerthoughts/', 'mildlyinteresting/'\n",
    "# # 'controversial', 'top', 'rising'\n",
    "# subreddits = ['science/', 'technology/', 'fitness/', 'sports/', 'showerthoughts/', \n",
    "#               'mildlyinteresting/']\n",
    "# filters = ['new'] \n",
    "# # The most number of subreddit posts are available in the 'new' filter. The default subreddit url\n",
    "# # brings up the 'hot' filter within the subreddit. As part of this analysis, I found that reddit\n",
    "# # splits all 'new' posts into 'hot', 'top', 'controversial' and 'rising'.\n",
    "# for subreddit in subreddits:\n",
    "#     posts = []\n",
    "#     print('Pulling posts from:', subreddit, '...')\n",
    "#     print()\n",
    "#     for filter in filters: \n",
    "#         print()\n",
    "#         print('Pulling filter:', filter, '...')\n",
    "#         print()\n",
    "#         url = 'https://www.reddit.com/r/' + subreddit + filter + '.json'\n",
    "#         after = None\n",
    "#         agent = {'User-agent': 'red_bk'}\n",
    "#         num_requests = 100 # Number of times to request posts from reddit's API\n",
    "#         for i in range(num_requests):\n",
    "#             if i % 10 == 0:\n",
    "#                 print('Requests made:', i)\n",
    "#             if after == None:\n",
    "#                 params = {} # this dict represents the unique tag that tells us the last post pulled\n",
    "#             else:\n",
    "#                 params = {'after': after}\n",
    "#             res = requests.get(url, headers=agent, params=params)\n",
    "#             if res.status_code == 200:\n",
    "#                 the_json = res.json()\n",
    "#                 posts.extend(the_json['data']['children'])\n",
    "#                 after = the_json['data']['after']\n",
    "#                 if i % 5 == 0:\n",
    "#                     agent['User-agent'] = 'red_bk' + str(i)\n",
    "#             else:\n",
    "#                 print('Error:', status_code)\n",
    "#                 break\n",
    "#         # Convert subreddit list of dictionaries to posts\n",
    "#     df = pd.DataFrame(posts)\n",
    "#     df.to_csv('./data/' + subreddit[:-1] + '_' + time.strftime('%Y-%m-%d-%I%p'), index=False)\n",
    "#     time.sleep(1)\n",
    "\n",
    "# time_elapsed_secs = time.time() - t0 # time elapsed in seconds\n",
    "# print()\n",
    "# print('Num of Posts Collected (last subreddit):', len(posts))\n",
    "# print('Time Elapsed:', datetime.timedelta(seconds=time_elapsed_secs))\n",
    "# print('Code ended at:', time.strftime('%Y-%m-%d-%I%p:%Mmins'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Gathered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shower_df = pd.read_csv('./data/showerthoughts_2018-12-17-03PM')\n",
    "science_df = pd.read_csv('./data/science_2018-12-17-03PM')\n",
    "tech_df = pd.read_csv('./data/technology_2018-12-17-03PM')\n",
    "fit_df = pd.read_csv('./data/fitness_2018-12-17-03PM')\n",
    "sports_df = pd.read_csv('./data/sports_2018-12-17-03PM')\n",
    "mild_df = pd.read_csv('./data/mildlyinteresting_2018-12-17-03PM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dfs = [science_df, tech_df, fit_df, sports_df, mild_df, shower_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionaries stored as strings back to dictionaries\n",
    "for df in list_of_dfs:\n",
    "    df['data'] = df['data'].map(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check type of dict entry\n",
    "# type(science_df['data'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "science Num of Unique Posts: 870\n",
      "technology Num of Unique Posts: 536\n",
      "Fitness Num of Unique Posts: 914\n",
      "sports Num of Unique Posts: 498\n",
      "mildlyinteresting Num of Unique Posts: 998\n",
      "Showerthoughts Num of Unique Posts: 998\n"
     ]
    }
   ],
   "source": [
    "# Num of unique posts in each subreddit (using 'name' for each post as an identifier)\n",
    "\n",
    "for df in list_of_dfs:\n",
    "    sr_name = df['data'][0]['subreddit']\n",
    "    df['post_name'] = df['data'].map(lambda x: x['name'])\n",
    "    print(sr_name, 'Num of Unique Posts:', len(set(df['post_name'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>kind</th>\n",
       "      <th>post_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'scienc...</td>\n",
       "      <td>t3</td>\n",
       "      <td>t3_a74sy0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'scienc...</td>\n",
       "      <td>t3</td>\n",
       "      <td>t3_a74lvx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'scienc...</td>\n",
       "      <td>t3</td>\n",
       "      <td>t3_a74jhp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'scienc...</td>\n",
       "      <td>t3</td>\n",
       "      <td>t3_a74bl1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'scienc...</td>\n",
       "      <td>t3</td>\n",
       "      <td>t3_a74aro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data kind  post_name\n",
       "0  {'approved_at_utc': None, 'subreddit': 'scienc...   t3  t3_a74sy0\n",
       "1  {'approved_at_utc': None, 'subreddit': 'scienc...   t3  t3_a74lvx\n",
       "2  {'approved_at_utc': None, 'subreddit': 'scienc...   t3  t3_a74jhp\n",
       "3  {'approved_at_utc': None, 'subreddit': 'scienc...   t3  t3_a74bl1\n",
       "4  {'approved_at_utc': None, 'subreddit': 'scienc...   t3  t3_a74aro"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "science_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are duplicate posts within each df (reddit provides duplicates once all the \"new\" posts\n",
    "# according to reddit have been provided.)\n",
    "for df in list_of_dfs:\n",
    "    df.drop_duplicates(subset='post_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "science Rows: 870\n",
      "technology Rows: 536\n",
      "Fitness Rows: 914\n",
      "sports Rows: 498\n",
      "mildlyinteresting Rows: 998\n",
      "Showerthoughts Rows: 998\n"
     ]
    }
   ],
   "source": [
    "# Check to make sure duplicates were dropped correctly\n",
    "for df in list_of_dfs:\n",
    "    sr_name = df['data'][0]['subreddit']\n",
    "    print(sr_name, 'Rows:', df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Subreddits to compare: r/science and r/technology_ <br>\n",
    "- _Both are relatively similar, however, science focuses on academic research while technology is more news-oriented._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine science and technology dfs\n",
    "df_st = pd.concat([science_df, tech_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index range after combining dfs \n",
    "df_st.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=1406, step=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st.index # Check index range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by cleaning posts from the r/science and r/technology subreddits. These subreddits are relatively similar in terms of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the dictionary entries for each post into individual columns \n",
    "\n",
    "# Combined df\n",
    "\n",
    "dict_keys = sorted(df_st['data'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96    0.540541\n",
       "92    0.369844\n",
       "94    0.086771\n",
       "99    0.000711\n",
       "97    0.000711\n",
       "93    0.000711\n",
       "90    0.000711\n",
       "Name: data, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of keys (i.e. data fields) for each post\n",
    "(df_st['data'].map(lambda x: len(x.keys()))).value_counts(normalize=True)\n",
    "\n",
    "# Most posts have a similar number of keys. The keys missing in certain posts are unlikely to\n",
    "# be significant features for this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['approved_at_utc', 'approved_by', 'archived', 'author', 'author_flair_background_color', 'author_flair_css_class', 'author_flair_richtext', 'author_flair_template_id', 'author_flair_text', 'author_flair_text_color', 'author_flair_type', 'author_fullname', 'author_patreon_flair', 'banned_at_utc', 'banned_by', 'can_gild', 'can_mod_post', 'category', 'clicked', 'content_categories', 'contest_mode', 'created', 'created_utc', 'distinguished', 'domain', 'downs', 'edited', 'gilded', 'gildings', 'hidden', 'hide_score', 'id', 'is_crosspostable', 'is_meta', 'is_original_content', 'is_reddit_media_domain', 'is_robot_indexable', 'is_self', 'is_video', 'likes', 'link_flair_background_color', 'link_flair_css_class', 'link_flair_richtext', 'link_flair_template_id', 'link_flair_text', 'link_flair_text_color', 'link_flair_type', 'locked', 'media', 'media_embed', 'media_only', 'mod_note', 'mod_reason_by', 'mod_reason_title', 'mod_reports', 'name', 'no_follow', 'num_comments', 'num_crossposts', 'num_reports', 'over_18', 'parent_whitelist_status', 'permalink', 'pinned', 'post_hint', 'preview', 'pwls', 'quarantine', 'removal_reason', 'report_reasons', 'saved', 'score', 'secure_media', 'secure_media_embed', 'selftext', 'selftext_html', 'send_replies', 'spoiler', 'stickied', 'subreddit', 'subreddit_id', 'subreddit_name_prefixed', 'subreddit_subscribers', 'subreddit_type', 'suggested_sort', 'thumbnail', 'thumbnail_height', 'thumbnail_width', 'title', 'ups', 'url', 'user_reports', 'view_count', 'visited', 'whitelist_status', 'wls']\n"
     ]
    }
   ],
   "source": [
    "# List of data fields within each post\n",
    "print(dict_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the data fields above shows that many of them contain data for admin/formatting purposes, as opposed to information on the content of the post (e.g. \"author_flair_background_color\", \"is_robot_indexable\", etc.) that might allude to which subreddit it belongs to. Thus, I will select a subset of these data fields to examine in further detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected data fields\n",
    "\n",
    "post_fields = ['approved_by', 'author', 'category', 'content_categories', 'created', 'domain', \n",
    "              'likes', 'media', 'name', 'num_comments', 'num_crossposts', 'num_reports','selftext',\n",
    "               'subreddit', 'title', 'wls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in post_fields:\n",
    "    df_st[field] = df_st['data'].map(lambda x: x[field])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the selected columns in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>data</th>\n",
       "      <th>kind</th>\n",
       "      <th>post_name</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>content_categories</th>\n",
       "      <th>created</th>\n",
       "      <th>domain</th>\n",
       "      <th>likes</th>\n",
       "      <th>media</th>\n",
       "      <th>name</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>wls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'scienc...</td>\n",
       "      <td>t3</td>\n",
       "      <td>t3_a74sy0</td>\n",
       "      <td>None</td>\n",
       "      <td>Wagamaga</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.545115e+09</td>\n",
       "      <td>news.vcu.edu</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_a74sy0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>science</td>\n",
       "      <td>Children of parents who have alcohol use disor...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'scienc...</td>\n",
       "      <td>t3</td>\n",
       "      <td>t3_a74lvx</td>\n",
       "      <td>None</td>\n",
       "      <td>cromatron</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.545114e+09</td>\n",
       "      <td>gizmodo.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_a74lvx</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>science</td>\n",
       "      <td>Astronomers Just Discovered 'Farout,' the Most...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'scienc...</td>\n",
       "      <td>t3</td>\n",
       "      <td>t3_a74jhp</td>\n",
       "      <td>None</td>\n",
       "      <td>ididntwin</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.545113e+09</td>\n",
       "      <td>nature.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_a74jhp</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>science</td>\n",
       "      <td>A novel and safe small molecule enhances hair ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'scienc...</td>\n",
       "      <td>t3</td>\n",
       "      <td>t3_a74bl1</td>\n",
       "      <td>None</td>\n",
       "      <td>Wagamaga</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.545112e+09</td>\n",
       "      <td>ldi.upenn.edu</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_a74bl1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>science</td>\n",
       "      <td>A study of 500 U.S. hospitals, and from the pe...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'scienc...</td>\n",
       "      <td>t3</td>\n",
       "      <td>t3_a74aro</td>\n",
       "      <td>None</td>\n",
       "      <td>Edude60</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.545112e+09</td>\n",
       "      <td>nasa.gov</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_a74aro</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>science</td>\n",
       "      <td>Saturn's Rings May Disappear in 100 million years</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               data kind  post_name  \\\n",
       "0      0  {'approved_at_utc': None, 'subreddit': 'scienc...   t3  t3_a74sy0   \n",
       "1      1  {'approved_at_utc': None, 'subreddit': 'scienc...   t3  t3_a74lvx   \n",
       "2      2  {'approved_at_utc': None, 'subreddit': 'scienc...   t3  t3_a74jhp   \n",
       "3      3  {'approved_at_utc': None, 'subreddit': 'scienc...   t3  t3_a74bl1   \n",
       "4      4  {'approved_at_utc': None, 'subreddit': 'scienc...   t3  t3_a74aro   \n",
       "\n",
       "  approved_by     author category content_categories       created  \\\n",
       "0        None   Wagamaga     None               None  1.545115e+09   \n",
       "1        None  cromatron     None               None  1.545114e+09   \n",
       "2        None  ididntwin     None               None  1.545113e+09   \n",
       "3        None   Wagamaga     None               None  1.545112e+09   \n",
       "4        None    Edude60     None               None  1.545112e+09   \n",
       "\n",
       "          domain likes media       name  num_comments  num_crossposts  \\\n",
       "0   news.vcu.edu  None  None  t3_a74sy0             3               0   \n",
       "1    gizmodo.com  None  None  t3_a74lvx             3               0   \n",
       "2     nature.com  None  None  t3_a74jhp             3               0   \n",
       "3  ldi.upenn.edu  None  None  t3_a74bl1            14               0   \n",
       "4       nasa.gov  None  None  t3_a74aro             5               0   \n",
       "\n",
       "  num_reports selftext subreddit  \\\n",
       "0        None            science   \n",
       "1        None            science   \n",
       "2        None            science   \n",
       "3        None            science   \n",
       "4        None            science   \n",
       "\n",
       "                                               title  wls  \n",
       "0  Children of parents who have alcohol use disor...    6  \n",
       "1  Astronomers Just Discovered 'Farout,' the Most...    6  \n",
       "2  A novel and safe small molecule enhances hair ...    6  \n",
       "3  A study of 500 U.S. hospitals, and from the pe...    6  \n",
       "4  Saturn's Rings May Disappear in 100 million years    6  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>data</th>\n",
       "      <th>kind</th>\n",
       "      <th>post_name</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>content_categories</th>\n",
       "      <th>created</th>\n",
       "      <th>domain</th>\n",
       "      <th>likes</th>\n",
       "      <th>media</th>\n",
       "      <th>name</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>num_reports</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>wls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>531</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'techno...</td>\n",
       "      <td>t3</td>\n",
       "      <td>t3_a4wyyf</td>\n",
       "      <td>None</td>\n",
       "      <td>khayrirrw</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.544487e+09</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_a4wyyf</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>technology</td>\n",
       "      <td>Chinese Court Says Apple Infringed on Qualcomm...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>532</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'techno...</td>\n",
       "      <td>t3</td>\n",
       "      <td>t3_a4wsh4</td>\n",
       "      <td>None</td>\n",
       "      <td>chrisarchitect</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.544486e+09</td>\n",
       "      <td>reuters.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_a4wsh4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>technology</td>\n",
       "      <td>China says rejecting physical cash is illegal ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>533</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'techno...</td>\n",
       "      <td>t3</td>\n",
       "      <td>t3_a4wgsz</td>\n",
       "      <td>None</td>\n",
       "      <td>swingadmin</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.544484e+09</td>\n",
       "      <td>arstechnica.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_a4wgsz</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>technology</td>\n",
       "      <td>Elon Musk makes mockery of SEC settlement in 6...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>534</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'techno...</td>\n",
       "      <td>t3</td>\n",
       "      <td>t3_a4wc4a</td>\n",
       "      <td>None</td>\n",
       "      <td>speckz</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.544483e+09</td>\n",
       "      <td>techdirt.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_a4wc4a</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>technology</td>\n",
       "      <td>AT&amp;amp;T Finds Yet Another Way To Nickel-And-D...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>535</td>\n",
       "      <td>{'approved_at_utc': None, 'subreddit': 'techno...</td>\n",
       "      <td>t3</td>\n",
       "      <td>t3_a4w8rd</td>\n",
       "      <td>None</td>\n",
       "      <td>spsheridan</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.544482e+09</td>\n",
       "      <td>venturebeat.com</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>t3_a4w8rd</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>technology</td>\n",
       "      <td>Qualcomm wins iPhone import and sales ban in C...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               data kind  \\\n",
       "1401    531  {'approved_at_utc': None, 'subreddit': 'techno...   t3   \n",
       "1402    532  {'approved_at_utc': None, 'subreddit': 'techno...   t3   \n",
       "1403    533  {'approved_at_utc': None, 'subreddit': 'techno...   t3   \n",
       "1404    534  {'approved_at_utc': None, 'subreddit': 'techno...   t3   \n",
       "1405    535  {'approved_at_utc': None, 'subreddit': 'techno...   t3   \n",
       "\n",
       "      post_name approved_by          author category content_categories  \\\n",
       "1401  t3_a4wyyf        None       khayrirrw     None               None   \n",
       "1402  t3_a4wsh4        None  chrisarchitect     None               None   \n",
       "1403  t3_a4wgsz        None      swingadmin     None               None   \n",
       "1404  t3_a4wc4a        None          speckz     None               None   \n",
       "1405  t3_a4w8rd        None      spsheridan     None               None   \n",
       "\n",
       "           created           domain likes media       name  num_comments  \\\n",
       "1401  1.544487e+09      nytimes.com  None  None  t3_a4wyyf             4   \n",
       "1402  1.544486e+09      reuters.com  None  None  t3_a4wsh4             8   \n",
       "1403  1.544484e+09  arstechnica.com  None  None  t3_a4wgsz            17   \n",
       "1404  1.544483e+09     techdirt.com  None  None  t3_a4wc4a             8   \n",
       "1405  1.544482e+09  venturebeat.com  None  None  t3_a4w8rd             1   \n",
       "\n",
       "      num_crossposts num_reports selftext   subreddit  \\\n",
       "1401               0        None           technology   \n",
       "1402               0        None           technology   \n",
       "1403               1        None           technology   \n",
       "1404               0        None           technology   \n",
       "1405               0        None           technology   \n",
       "\n",
       "                                                  title  wls  \n",
       "1401  Chinese Court Says Apple Infringed on Qualcomm...    6  \n",
       "1402  China says rejecting physical cash is illegal ...    6  \n",
       "1403  Elon Musk makes mockery of SEC settlement in 6...    6  \n",
       "1404  AT&amp;T Finds Yet Another Way To Nickel-And-D...    6  \n",
       "1405  Qualcomm wins iPhone import and sales ban in C...    6  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind 0\n",
      "post_name 0\n",
      "approved_by 1406\n",
      "author 0\n",
      "category 1406\n",
      "content_categories 1406\n",
      "created 0\n",
      "domain 0\n",
      "likes 1406\n",
      "media 1406\n",
      "name 0\n",
      "num_comments 0\n",
      "num_crossposts 0\n",
      "num_reports 1406\n",
      "selftext 0\n",
      "subreddit 0\n",
      "title 0\n",
      "wls 0\n"
     ]
    }
   ],
   "source": [
    "# Check for null values within columns in df\n",
    "for col in df_st.columns[2:]:\n",
    "    print(col, df_st[col].isnull().sum())\n",
    "    \n",
    "# Columns with null values will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null columns and original data columns\n",
    "df_st.drop(columns=['approved_by', 'category', 'content_categories', 'likes', 'media',\n",
    "                         'num_reports', 'kind'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most values in column 'selftext' appear to be empty strings, and the cells with values don't \n",
    "# appear to be good candidates for feature selection. Thus, let's remove 'selftext' as well.\n",
    "df_st['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st.drop(columns='selftext', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'data', 'post_name', 'author', 'created', 'domain', 'name',\n",
       "       'num_comments', 'num_crossposts', 'subreddit', 'title', 'wls'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Along with 'title', 'author' and 'domain' may be potentially good features to help our classifier predict which subreddit a post belongs to. However, I will begin with conducting NLP solely on the post's 'title'._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the draft list of candidates for feature selection for the r/science subreddit.\n",
    "X = df_st['title']\n",
    "y = df_st['subreddit'] # The target value is the column with the subreddit's name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Children of parents who have alcohol use disor...\n",
       "1    Astronomers Just Discovered 'Farout,' the Most...\n",
       "2    A novel and safe small molecule enhances hair ...\n",
       "3    A study of 500 U.S. hospitals, and from the pe...\n",
       "4    Saturn's Rings May Disappear in 100 million years\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "science       870\n",
       "technology    536\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target classes\n",
    "y = y.map({'science': 0, 'technology': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.618777\n",
       "1    0.381223\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_For this dataset, the baseline accuracy of any model is 61.9%. This is the accuracy my models must beat._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling with NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Vectorizers\n",
    "\n",
    "count_vect = CountVectorizer() # CountVectorizer requires a Series as an input, *not* df.\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Model\n",
    "logreg = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier 1: Logisitic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline 1: CountVectorizer + LogisticRegression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Pipeline 1\n",
    "pipe1 = Pipeline([\n",
    "    ('count_vect', count_vect),\n",
    "    ('logreg', logreg)  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bala_K/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training Set Score: 0.9193548387096774\n",
      "Best Parameters: {'count_vect__max_features': None, 'count_vect__ngram_range': (1, 3), 'count_vect__stop_words': 'english'}\n",
      "Test Set Score: 0.9147727272727273\n"
     ]
    }
   ],
   "source": [
    "# Tune parameters and evaluate model\n",
    "params = {\n",
    "    'count_vect__stop_words': ['english'],\n",
    "    'count_vect__max_features': [None, 6000],\n",
    "    'count_vect__ngram_range': [(1,3), (1,4)],\n",
    "#     'logreg__C': [0.6, 0.8, 1.0] # C = 1.0 was the best parameter\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe1, param_grid=params, cv=5)\n",
    "gs.fit(X_train, y_train)\n",
    "print('Best Training Set Score:', gs.best_score_)\n",
    "print('Best Parameters:', gs.best_params_)\n",
    "print('Test Set Score:', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The first pipeline (vectorizer + model) performs quite well relative to the baseline accuracy: 91% (training and test set) vs 62%. This means that our model predicts the right subreddit (in this case, r/science or r/technology 9 out of 10 times)._ <br> <br>\n",
    "_Let's see what we can do to improve model performance._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline 2: Tfidf + Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Pipeline 2\n",
    "pipe2 = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('logreg', logreg)  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bala_K/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training Set Score: 0.8425047438330171\n",
      "Best Parameters: {'logreg__penalty': 'l2', 'tfidf__max_features': None, 'tfidf__stop_words': None}\n",
      "Test Set Score: 0.8721590909090909\n"
     ]
    }
   ],
   "source": [
    "# Tune parameters and evaluate model\n",
    "params = {\n",
    "    'tfidf__stop_words': [None, 'english'],\n",
    "    'tfidf__max_features': [None, 5000, 6000],\n",
    "    'logreg__penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe2, param_grid=params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print('Best Training Set Score:', gs.best_score_)\n",
    "print('Best Parameters:', gs.best_params_)\n",
    "print('Test Set Score:', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_With logisitic regression, CountVectorizer yields better results than the TFIDF vectorizer._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline 3: CountVectorizer + RandomForest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Let's see if an ensemble method such as RandomForest will yield better results..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up pipeline 3\n",
    "pipe3 = Pipeline([\n",
    "    ('count_vect', count_vect),\n",
    "    ('rf', rf)  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training Set Score: 0.8795066413662239\n",
      "Best Parameters: {'count_vect__max_features': 1000, 'count_vect__stop_words': 'english', 'rf__max_depth': None, 'rf__n_estimators': 500}\n",
      "Test Set Score: 0.8806818181818182\n",
      "\n",
      "Time Elapsed: 0:01:17.488705\n",
      "Code ended at: 04PM:44mins\n"
     ]
    }
   ],
   "source": [
    "# Tune parameters and evaluate model\n",
    "\n",
    "## Set timer\n",
    "t0 = time.time()\n",
    "\n",
    "params = {\n",
    "    'count_vect__stop_words': ['english'],\n",
    "    'count_vect__max_features': [950, 1000, 1050],\n",
    "    'rf__n_estimators': [475, 500, 525],\n",
    "    'rf__max_depth': [None, 5, 6]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe3, param_grid=params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print('Best Training Set Score:', gs.best_score_)\n",
    "print('Best Parameters:', gs.best_params_)\n",
    "print('Test Set Score:', gs.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "time_elapsed_secs = time.time() - t0 # time elapsed in seconds\n",
    "print('Time Elapsed:', datetime.timedelta(seconds=time_elapsed_secs))\n",
    "print('Code ended at:', time.strftime('%I:%M %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Iterations on the parameters above show that LogReg is still the best performing model between the two_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline 4: TFIDF + RandomForest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up pipeline 4\n",
    "pipe4 = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('rf', rf)  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training Set Score: 0.8795066413662239\n",
      "Best Parameters: {'rf__max_depth': None, 'rf__n_estimators': 500, 'tfidf__max_features': 1000, 'tfidf__stop_words': 'english'}\n",
      "Test Set Score: 0.8693181818181818\n",
      "\n",
      "Time Elapsed: 0:01:20.696398\n",
      "Code ended at: 04PM:47mins\n"
     ]
    }
   ],
   "source": [
    "# Tune parameters and evaluate model\n",
    "\n",
    "## Set timer\n",
    "t0 = time.time()\n",
    "\n",
    "params = {\n",
    "    'tfidf__stop_words': ['english'],\n",
    "    'tfidf__max_features': [800, 1000, 1200],\n",
    "    'rf__n_estimators': [450, 500, 550],\n",
    "    'rf__max_depth': [None, 2, 3]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe4, param_grid=params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print('Best Training Set Score:', gs.best_score_)\n",
    "print('Best Parameters:', gs.best_params_)\n",
    "print('Test Set Score:', gs.score(X_test, y_test))\n",
    "print()\n",
    "\n",
    "time_elapsed_secs = time.time() - t0 # time elapsed in seconds\n",
    "print('Time Elapsed:', datetime.timedelta(seconds=time_elapsed_secs))\n",
    "print('Code ended at:', time.strftime('%%I:%M %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_With RandomForest, CountVectorizer provides slightly better results that TFIDF. LogReg with CountVectorizer is still the best performing combo out of those evaluated, but there is still room for improvement._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Models using VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate voter\n",
    "voter = VotingClassifier([\n",
    "    ('lr', pipe1),\n",
    "    ('rf', pipe3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Bala_K/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/Bala_K/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.9895635673624289\n",
      "Test Score: 0.8409090909090909\n"
     ]
    }
   ],
   "source": [
    "voter.fit(X_train, y_train)\n",
    "print('Training Score:', voter.score(X_train, y_train))\n",
    "print('Test Score:', voter.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance with Other Subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of posts by subreddit as of June, 2017: https://gist.github.com/anonymous/ef075ee973dd5f883ae17729c147c1de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
